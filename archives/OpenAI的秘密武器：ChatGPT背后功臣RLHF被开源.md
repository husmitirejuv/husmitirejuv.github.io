来自 Hugging Face、加拿大蒙特利尔 Mila 研究所、网易伏羲 AI Lab 的研究人员从零开始复现了 OpenAI 的 RLHF pipeline，并罗列了**25个关键实施细节**。他们成功展示了随着模型规模的增加，响应质量显著提升的**scaling行为**，其中 2.8B 和 6.9B 的 Pythia 模型在性能上超过了 OpenAI 发布的 1.3B checkpoint。

## RLHF复现的独特之处

研究人员表示，他们的“配方”独特之处在于对 SFT、RM 和 PPO 使用了**单一的学习率**，这使得重现他们的工作变得更加简单。训练好的模型 checkpoint 和代码已被公开发布。

然而，想要重现 OpenAI 的 RLHF pipeline 并不容易，主要有以下几个原因：

- RL 和 RLHF 的许多微妙实现细节对训练稳定性有很大影响。
- 对于指令遵循任务，评估模型表现存在一定困难。
- 模型需要长时间的训练和迭代。

因此，Hugging Face 的这项工作选择从 OpenAI 早期的 RLHF 工作中入手，探寻其核心实现。

## RLHF的三个关键步骤

### 步骤1：训练 SFT（监督微调）策略

通过下一个词预测损失对预训练的 LLM 进行微调，使用基于人类示范的数据集。在这项复现工作中，研究人员使用了过滤后的 Reddit TL;DR 数据集，与 OpenAI 的工作保持一致。

### 步骤2：收集偏好对并训练 RM（奖励模型）

通过 SFT 策略采样不同完成序列，让人类标注员指出他们更偏好的序列。基于这些偏好数据，研究人员优化了交叉熵损失函数，训练出能够预测人类偏好的 RM。

### 步骤3：针对 RM 训练 RL（强化学习）策略

从 SFT 策略初始化，RL 策略根据 RM 对采样的完成序列给出奖励分数，同时加上一个 KL 惩罚项以防止过度偏离 SFT 策略。研究人员使用 PPO 算法最大化 RLHF 目标函数。

## 25个细节深度复现

研究人员从数据集到 SFT、RM 和 PPO 的训练过程，共介绍了 25 个复现细节，包括：

- **数据预处理阶段**：确保提示查询的截断方式与 OpenAI 一致，并在 reference completions 前后添加特定标记。
- **SFT阶段**：使用标准的下一个 token 预测损失，调整学习率以优化模型性能。
- **RM训练**：仅在 EOS token 处提取奖励，并优化验证准确率。
- **PPO训练**：通过奖励白化和长度控制分析，进一步提升模型性能。

## RLHF的研究意义

这项工作不仅展示了 RLHF 的复现细节，还通过实验验证了模型在不同规模下的性能表现。研究人员还对 RM 和 PPO 的训练行为进行了深入分析，为未来的研究提供了宝贵的参考。

👉 [WildCard | 一分钟注册，轻松订阅海外线上服务](https://bit.ly/bewildcard)

## 总结

Hugging Face 的 RLHF 复现工作为开源社区提供了重要的技术支持。通过公开代码和模型 checkpoint，研究人员降低了 RLHF 的复现门槛，为更多开发者探索强化学习与人类反馈的结合提供了可能。

---
**关键词**：RLHF, SFT, RM, PPO, OpenAI, ChatGPT, 强化学习, 奖励模型